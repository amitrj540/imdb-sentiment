{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WznagiAh3bEX"
   },
   "source": [
    "# Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdrGaf5Gto1S"
   },
   "source": [
    "**Data description :** This data contains 50K movie reviews posted on IMDB website. The data is divided into 2 folders `train` and `test` each containing 25K observation. `train` and `test` are further divided in to two folder `pos` (positive obs) and `neg` (negative obs) each containing 12.5K observations while consolidating the data into one file (i.e. single file containing all `train` and other containing all `test` data) I have put first 12.5K as `pos` observations and next 12.5K as `neg` observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYnPrO1s311o"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DmeeYt8xd5SY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Flow >> reading File line by line and stripping empty spaces of each line.\n",
    "test_set = [line.strip() for line in open(\"./data/movie_data/full_test.txt\", 'r')]\n",
    "train_set = [line.strip() for line in open(\"./data/movie_data/full_train.txt\", 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzFMA0wh4Cq3"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "129c0Gp34Leh"
   },
   "outputs": [],
   "source": [
    "## required packages \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2x-ckij1XgN"
   },
   "outputs": [],
   "source": [
    "#Flow >> iterating list, for each line removing all punctuation and brackers and replacing HTML tags with space.\n",
    "no_space = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "preprocess = lambda data : [space.sub(\" \", line) for line in [no_space.sub(\"\", line.lower()) for line in data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3I4IAAP8CIw"
   },
   "outputs": [],
   "source": [
    "test_set_clean = preprocess(test_set)\n",
    "train_set_clean = preprocess(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nh9iiILzrNv"
   },
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkDjPps2zuah",
    "outputId": "4ff43157-1d52-4e72-a48f-7c0a6ee76d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "## required packages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAfJwo1k0Gcp"
   },
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_C4kPD4XNVw"
   },
   "outputs": [],
   "source": [
    "#Flow >> Iterating list, for each line removing all stop words present in it (eg. in, of, at, a, the etc.)\n",
    "eng_stop_words = stopwords.words('english')\n",
    "rem_stopwords = lambda x :[\" \".join([word for word in line.split() if word not in eng_stop_words]) for line in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMEVB5jLsBkl"
   },
   "outputs": [],
   "source": [
    "no_stop_train = rem_stopwords(train_set_clean)\n",
    "no_stop_test = rem_stopwords(test_set_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbkk7BdFz08G"
   },
   "source": [
    "## Normalizing text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN_YHkjq0VZs"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGUIKJ99z4zA"
   },
   "outputs": [],
   "source": [
    "#required packages\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ_OLsbv0h-Q"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJPesBe70neH"
   },
   "outputs": [],
   "source": [
    "#Flow >> Iterating list, for each word in line, converting that word into root word. (eg. sleeping -> sleep)\n",
    "stem_text = lambda x : [\" \".join([stemmer.stem(word) for word in line.split()]) for line in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0E9vgrN1mDW"
   },
   "outputs": [],
   "source": [
    "train_stem = stem_text(no_stop_train)\n",
    "test_stem = stem_text(no_stop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdqeMX_lggVT"
   },
   "source": [
    "## Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNbuGd3SqglW"
   },
   "outputs": [],
   "source": [
    "#required packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkqYE4TDqsTY"
   },
   "outputs": [],
   "source": [
    "c_vect = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhAJKqPaqxL5"
   },
   "outputs": [],
   "source": [
    "c_vect.fit(train_stem)\n",
    "X = c_vect.transform(train_stem)\n",
    "X_test_set = c_vect.transform(test_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzmkrpG-2Iv7"
   },
   "source": [
    "## Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wo4zia62IOb"
   },
   "outputs": [],
   "source": [
    "#required packages\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qYfilUA2vLC"
   },
   "outputs": [],
   "source": [
    "X = maxabs_scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7prUTx6TspVk"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0sotwXnid-c"
   },
   "source": [
    "The aim is to built a model which can classify positive words (eg. great, good, excellent etc.) and negative words (eg. bad, worst, boring etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4U_F2iRNstK1"
   },
   "outputs": [],
   "source": [
    "#required packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIq-EBhKs5vm"
   },
   "outputs": [],
   "source": [
    "#it will be same for train as well as test (reason mentioned at the top)\n",
    "y = [1 if i < 12500 else 0 for i in range(25000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6XyZ1qEq7Dw"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_IQD0axxpxl",
    "outputId": "ee34a292-26b0-4d1c-cc4b-9d51ebcc2ac2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#parameter tuning of LogisticRegression model \n",
    "#inverse regularization parameter - c\n",
    "\n",
    "score_dict = {}\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    lr_model = LogisticRegression(C=c)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    score_dict.setdefault(c, accuracy_score(lr_model.predict(X_test), y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sohx4Kvl5jL-",
    "outputId": "c4603312-3fcf-4ed6-c2c4-f640d4ee3abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for c = 0.05 accuracy 0.8796363636363637 is maximum.\n"
     ]
    }
   ],
   "source": [
    "max_acc = [(c, acc) for c, acc in sorted(score_dict.items(), key = lambda tup: tup[1], reverse=True)]\n",
    "\n",
    "print(f\"for c = {max_acc[0][0]} accuracy {max_acc[0][1]} is maximum.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XhTyJ3f-dKd"
   },
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dw9AFulN-POR",
    "outputId": "7d06c1f2-d80a-4d15-c723-8b429ea07919"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model_final = LogisticRegression(C=0.05)\n",
    "lr_model_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fypmV2ZtCRRh"
   },
   "source": [
    "## Predicting and testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kfO-Izz_jmW"
   },
   "outputs": [],
   "source": [
    "pred = lr_model_final.predict(X_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0MAHxA__-nU",
    "outputId": "8fb1592c-9048-4020-8e29-377bdfa771ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for final model on test data is 87.64 %\n"
     ]
    }
   ],
   "source": [
    "#as said earlier y is same for both train and test\n",
    "print(f\"Accuracy for final model on test data is {round(accuracy_score(y, pred)*100 , 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fDCRbqfO8QR"
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldvujl3YO_Qg"
   },
   "outputs": [],
   "source": [
    "# when multi_class='multinomial', coef_ corresponds to outcome 1 (True)\n",
    "# and -coef_ corresponds to outcome 0 (False).\n",
    "\n",
    "# both lr_model_final and c_vect is trained on train_stem \n",
    "# that's why feature_name in c_vect will corrospond to coef in lr_model_final\n",
    "\n",
    "feat_coeff = {word : coeff for word, coeff in zip(c_vect.get_feature_names(),lr_model_final.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcT4qrS5Vy3m"
   },
   "outputs": [],
   "source": [
    "pos_neg = sorted(feat_coeff.items(), key= lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZH14l7dhNqH"
   },
   "source": [
    "**Top 5 positive words :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFdwcTKOguen",
    "outputId": "7c0a2c85-dfb7-4856-d29c-a4db26075199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excel :: 0.9626706903838785\n",
      "perfect :: 0.7808248316882564\n",
      "favorit :: 0.7215479650741811\n",
      "great :: 0.6526663723624709\n",
      "refresh :: 0.6317573415665363\n"
     ]
    }
   ],
   "source": [
    "for i,j in pos_neg[:5]:\n",
    "    print(f\"{i} :: {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emlG6bIghXe4"
   },
   "source": [
    "**Top 5 negative words :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oznmxaXZhWs-",
    "outputId": "561cc211-b8a6-43b0-81da-e04998a8d416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst :: -1.3323651866026152\n",
      "wast :: -1.1568194985169875\n",
      "aw :: -1.069636256865342\n",
      "poorli :: -0.8897462546755232\n",
      "bore :: -0.828017672610324\n"
     ]
    }
   ],
   "source": [
    "for i,j in pos_neg[-5:][::-1]:\n",
    "    print(f\"{i} :: {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxnixgfViFPF"
   },
   "source": [
    "Output looks pretty much sane as it is correctly giving more +ve weights to positive words and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6morv_PqoJH"
   },
   "outputs": [],
   "source": [
    "rev = [\"didnt really liked the movie\",\"i really liked the movie it was great\", \"simran i love you like i hate you dammit\", \"movie was just okay\"]\n",
    "testing = c_vect.transform(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhB30c2EtSEK"
   },
   "outputs": [],
   "source": [
    "f=lr_model_final.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irLgb47ktXfv",
    "outputId": "cb5b443c-5fa4-4a8d-8061-9111e2fe258d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> didnt really liked the movie >>> 0\n",
      ">>> i really liked the movie it was great >>> 1\n",
      ">>> simran i love you like i hate you dammit >>> 1\n",
      ">>> movie was just okay >>> 0\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(rev,f):\n",
    "    print(f\">>> {i} >>> {j}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "imdb-reviews-sentiment-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
